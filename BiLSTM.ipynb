{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MjG9JyeqrHUK"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiangdiChai/nlp-cw-code-repo/blob/master/BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yez16VCrHT9",
        "outputId": "5fcb48af-abcf-4a12-942d-7c510d70f0fa"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "#! pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-26 15:45:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-26 15:45:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-26 15:45:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.94MB/s    in 6m 52s  \n",
            "\n",
            "2021-02-26 15:52:34 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjfizNEbrHT_"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBHG3pourHUA"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2abxaIrHUA"
      },
      "source": [
        "# Load data\n",
        "#feel free to edit the path\n",
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNbnmuwUrHUA"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sORELdZurHUB"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    print(\"Training model.\")\n",
        "    best_loss = 10\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "        \n",
        "        for batch in train_iter:\n",
        "            \n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            \n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "        \n",
        "        if valid_loss < best_loss:\n",
        "          best_loss = valid_loss \n",
        "          \n",
        "          torch.save(model.state_dict(),  './sample_data/BiLSTM_model.pth')\n",
        "          \n",
        "          print('model save ', best_loss)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.4f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XlWwiQgrHUB"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aw50Uw-rHUC"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCtDWgkgsJjV"
      },
      "source": [
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "\r\n",
        "def create_vocab(data):\r\n",
        "    \"\"\"\r\n",
        "    Creating a corpus of all the tokens used\r\n",
        "    \"\"\"\r\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\r\n",
        "\r\n",
        "    vocabulary = []\r\n",
        "\r\n",
        "    for sentence in data:\r\n",
        "        \r\n",
        "        sentence = sentence.lower() #lower case\r\n",
        "        sentence = sentence.translate(str.maketrans('', '', string.punctuation)) #remove punctuation\r\n",
        "        sentence = re.sub(r'\\d+', '', sentence) #remove number\r\n",
        "        \r\n",
        "        tokenized_sentence = []\r\n",
        "        \r\n",
        "        for token in sentence.split(' '): \r\n",
        "\r\n",
        "            tokenized_sentence.append(token)\r\n",
        "\r\n",
        "            if token not in vocabulary:\r\n",
        "                \r\n",
        "                vocabulary.append(token)\r\n",
        "\r\n",
        "\r\n",
        "        tokenized_corpus.append(tokenized_sentence)\r\n",
        "\r\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaSqxdDirHUD"
      },
      "source": [
        "def collate_fn_padd(batch):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "\n",
        "    batch_labels = [l for f, l in batch]\n",
        "    batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f, l in batch]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "\n",
        "    return seq_tensor, batch_labels\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eod23pZvrHUE"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        \n",
        "        self.hidden2label = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim),  #nn.Linear(hidden_dim * 2, 1)\n",
        "                          nn.LeakyReLU(),\n",
        "                          nn.Linear(hidden_dim, hidden_dim//2),\n",
        "                          nn.LeakyReLU(),\n",
        "                          nn.Linear(hidden_dim//2, 1),\n",
        "                          nn.LeakyReLU())\n",
        "       \n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "       \n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "        out = self.hidden2label(lstm_out[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx_FqFn3upni"
      },
      "source": [
        "\r\n",
        "def pre_processing(method):\r\n",
        "\r\n",
        "  training_origin_data = train_df['original']\r\n",
        "  training_edit_data = train_df['edit']\r\n",
        "  dev_origin_data = dev_df['original']\r\n",
        "  dev_edit_data = dev_df['edit']\r\n",
        "  test_origin_data = test_df['original']\r\n",
        "  test_edit_data = test_df['edit']\r\n",
        "\r\n",
        "  training_data = []\r\n",
        "  dev_data = []\r\n",
        "  test_data = []\r\n",
        "\r\n",
        "\r\n",
        "  #Replace the word in the bracket with the edit word \r\n",
        "  if method == 1: \r\n",
        "\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i].replace(training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")+1],training_edit_data[i]))\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i].replace(dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")+1],dev_edit_data[i]))\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      test_data.append(test_origin_data[i].replace(test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")+1],test_edit_data[i]))\r\n",
        "\r\n",
        "  #Show both original sentence and the edit sentence to the network\r\n",
        "  elif method == 2:\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i]+' '+training_origin_data[i].replace(training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")+1],training_edit_data[i]))\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i]+' '+dev_origin_data[i].replace(dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")+1],dev_edit_data[i]))\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      test_data.append(test_origin_data[i]+' '+test_origin_data[i].replace(test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")+1],test_edit_data[i]))\r\n",
        "  \r\n",
        "  #Append the edit word at the end of the original sentence.\r\n",
        "  elif method == 3:\r\n",
        "\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      training_data.append(training_origin_data[i]+' '+training_edit_data[i])\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      dev_data.append(dev_origin_data[i]+' '+dev_edit_data[i])  \r\n",
        "    for i in range(len(test_origin_data)):\r\n",
        "      test_data.append(test_origin_data[i]+' '+test_edit_data[i])\r\n",
        "\r\n",
        "  #Append the edit word after the word needs to be replaced. \r\n",
        "  elif method == 4:\r\n",
        "    for i in range(len(training_origin_data)): \r\n",
        "      word = training_origin_data[i][training_origin_data[i].find(\"<\") : training_origin_data[i].find(\">\")]\r\n",
        "      training_data.append(training_origin_data[i].replace(word, word+' '+training_edit_data[i]))\r\n",
        "\r\n",
        "    for i in range(len(dev_origin_data)): \r\n",
        "      word = dev_origin_data[i][dev_origin_data[i].find(\"<\") : dev_origin_data[i].find(\">\")]\r\n",
        "      dev_data.append(dev_origin_data[i].replace(word, word+' '+dev_edit_data[i]))\r\n",
        "\r\n",
        "    for i in range(len(test_origin_data)): \r\n",
        "      word = test_origin_data[i][test_origin_data[i].find(\"<\") : test_origin_data[i].find(\">\")]\r\n",
        "      test_data.append(test_origin_data[i].replace(word, word+' '+test_edit_data[i]))\r\n",
        "\r\n",
        "  return training_data,dev_data,test_data\r\n",
        "\r\n",
        "\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZU-9KL89bDS"
      },
      "source": [
        "## change preprocessing method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULArZ2Eu9sWf"
      },
      "source": [
        "training_data,dev_data,test_data = pre_processing(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtuahuKKyXDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa0f83e-f2e9-45a2-9579-5dae1ef95fd4"
      },
      "source": [
        "# Creating word vectors\r\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\r\n",
        "dev_vocab, dev_tokenized_corpus = create_vocab(dev_data)\r\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\r\n",
        "\r\n",
        "\r\n",
        "# Creating joint vocab from test and train:\r\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(training_data+dev_data+test_data)\r\n",
        "\r\n",
        "print(\"Vocab created.\")\r\n",
        "\r\n",
        "\r\n",
        "# We create representations for our tokens\r\n",
        "wvecs = [] # word vectors\r\n",
        "word2idx = [] # word2index\r\n",
        "idx2word = []\r\n",
        "\r\n",
        "# This is a large file, it will take a while to load in the memory!\r\n",
        "\r\n",
        "#Experiment with differet embedding dimension\r\n",
        "#with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f:\r\n",
        "#with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\r\n",
        "with codecs.open('glove.6B.200d.txt', 'r','utf-8') as f:\r\n",
        "#with codecs.open('glove.6B.300d.txt', 'r','utf-8') as f:\r\n",
        "  index = 0\r\n",
        "  for line in f.readlines():\r\n",
        "    # Ignore the first line - first line typically contains vocab, dimensionality\r\n",
        "    if len(line.strip().split()) > 3:\r\n",
        "      word = line.strip().split()[0]\r\n",
        "      if word in joint_vocab:\r\n",
        "          (word, vec) = (word,\r\n",
        "                     list(map(float,line.strip().split()[1:])))\r\n",
        "          wvecs.append(vec)\r\n",
        "          word2idx.append((word, index))\r\n",
        "          idx2word.append((index, word))\r\n",
        "          index += 1\r\n",
        "\r\n",
        "wvecs = np.array(wvecs)\r\n",
        "word2idx = dict(word2idx)\r\n",
        "idx2word = dict(idx2word)\r\n",
        "\r\n",
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in  training_tokenized_corpus]\r\n",
        "\r\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\r\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n",
        "\r\n",
        "dev_vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in dev_tokenized_corpus]\r\n",
        "dev_vectorized_seqs = [x if len(x) > 0 else [0] for x in dev_vectorized_seqs]\r\n",
        "\r\n",
        "\r\n",
        "test_vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in  test_tokenized_corpus]\r\n",
        "test_vectorized_seqs = [x if len(x) > 0 else [0] for x in test_vectorized_seqs]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAvjPBkiPlNW",
        "outputId": "ced9bcdf-0552-42f2-aba5-96cac01202e1"
      },
      "source": [
        "INPUT_DIM = len(word2idx)\r\n",
        "EMBEDDING_DIM = 200\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "model = BiLSTM(EMBEDDING_DIM, 256, INPUT_DIM, BATCH_SIZE, device)\r\n",
        "print(\"Model initialised.\")\r\n",
        "\r\n",
        "model.to(device)\r\n",
        "# We provide the model with our embeddings\r\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\r\n",
        "\r\n",
        "feature = vectorized_seqs\r\n",
        "dev_feature = dev_vectorized_seqs\r\n",
        "\r\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\r\n",
        "training = Task1Dataset(feature, train_df['meanGrade'])\r\n",
        "dev = Task1Dataset(dev_feature, dev_df['meanGrade'])\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(training, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "dev_loader = torch.utils.data.DataLoader(dev, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "print(\"Dataloaders created.\")\r\n",
        "\r\n",
        "loss_fn = nn.MSELoss()\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0005)\r\n",
        "\r\n",
        "train(train_loader, dev_loader, model, epochs)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Dataloaders created.\n",
            "Training model.\n",
            "model save  0.34302689283531823\n",
            "| Epoch: 01 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.5857 |\n",
            "model save  0.3054742734754514\n",
            "| Epoch: 02 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.5527 |\n",
            "model save  0.29814197924473207\n",
            "| Epoch: 03 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |         Val. Loss: 0.30 | Val. MSE: 0.30 |  Val. RMSE: 0.5460 |\n",
            "| Epoch: 04 | Train Loss: 0.21 | Train MSE: 0.21 | Train RMSE: 0.46 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.5774 |\n",
            "| Epoch: 05 | Train Loss: 0.18 | Train MSE: 0.18 | Train RMSE: 0.42 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.5710 |\n",
            "| Epoch: 06 | Train Loss: 0.15 | Train MSE: 0.15 | Train RMSE: 0.38 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.6128 |\n",
            "| Epoch: 07 | Train Loss: 0.12 | Train MSE: 0.12 | Train RMSE: 0.35 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.6014 |\n",
            "| Epoch: 08 | Train Loss: 0.11 | Train MSE: 0.11 | Train RMSE: 0.33 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.6217 |\n",
            "| Epoch: 09 | Train Loss: 0.09 | Train MSE: 0.09 | Train RMSE: 0.30 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.6168 |\n",
            "| Epoch: 10 | Train Loss: 0.08 | Train MSE: 0.08 | Train RMSE: 0.29 |         Val. Loss: 0.41 | Val. MSE: 0.41 |  Val. RMSE: 0.6424 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTkD9IOYUBRd"
      },
      "source": [
        "#get the prediction for test dataset \r\n",
        "\r\n",
        "test_feature = test_vectorized_seqs\r\n",
        "\r\n",
        "test = Task1Dataset(test_feature, train_df['meanGrade'][0:len(test_feature)])\r\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n",
        "\r\n",
        "predictions = []\r\n",
        "best_model = BiLSTM(EMBEDDING_DIM, 256, INPUT_DIM, BATCH_SIZE, device)\r\n",
        "best_model.load_state_dict(torch.load('/content/sample_data/BiLSTM_model.pth'))\r\n",
        "\r\n",
        "best_model.eval()\r\n",
        "with torch.no_grad():\r\n",
        "  for batch in test_loader:\r\n",
        "    feature, target = batch\r\n",
        "    feature, target = feature.to(device), target.to(device)\r\n",
        "    feature = torch.LongTensor(feature)\r\n",
        "    best_model.batch_size = target.shape[0]\r\n",
        "    best_model.hidden = best_model.init_hidden()\r\n",
        "    prediction = best_model(feature).squeeze(1)\r\n",
        "    pred = prediction.detach().cpu().numpy()  \r\n",
        "    pred = np.reshape(pred,(len(pred),1))\r\n",
        "    for p in pred:\r\n",
        "      predictions.append(p)\r\n",
        "\r\n",
        "predictions = np.asarray(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-H-rhFiLun"
      },
      "source": [
        "#write out csv file for the predict result\r\n",
        "import csv\r\n",
        "out_loc = '/content/sample_data/task-1-output.csv'\r\n",
        "with open(out_loc, \"w\") as f:\r\n",
        "    writer = csv.writer(f)\r\n",
        "    writer.writerow(('id','pred'))\r\n",
        "    for i in range(len(test_df['id'])):\r\n",
        "        writer.writerow((test_df['id'][i],predictions[i][0]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
