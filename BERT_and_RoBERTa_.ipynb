{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "BERT_and_RoBERTa .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiangdiChai/nlp-cw-code-repo/blob/master/BERT_and_RoBERTa_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j8bQt_39dpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b061c52-35b3-48a8-8a61-554f42355f44"
      },
      "source": [
        "# You will need to download any word embeddings required for your code, e.g.:\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip\n",
        "\n",
        "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weTOWm1M9dpn"
      },
      "source": [
        "# Imports packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoLZMFdz9dpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28790218-52c1-4124-c625-a6a20afb8366"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "cuda_dev = '0'\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(f'Using {device}')\n",
        "\n",
        "if use_cuda:\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda:0\n",
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVMMI7hJxwFe"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "val_df = pd.read_csv('dev.csv') \n",
        "test_df = pd.read_csv('test.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFbfnmdQeeug"
      },
      "source": [
        "# define hyperparameter\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 32\n",
        "learning_rate = 3e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge2odzpgRqaw"
      },
      "source": [
        "### preprocessing data\n",
        "\n",
        "\n",
        "## find the start and end position of the key word in original sentence\n",
        "def find_key_word(original):\n",
        "  start_position = original.find('<')\n",
        "  end_position = original.find('>')\n",
        "  return start_position, end_position\n",
        "\n",
        "## replace the word with the substitution word\n",
        "def replace_key_word(original,edit):\n",
        "  start_position, end_position = find_key_word(original)\n",
        "  result = original.replace(original[start_position : end_position+1],edit)\n",
        "  return result\n",
        "\n",
        "# extract the substitution word\n",
        "def extract_key_word(original):\n",
        "  start_position, end_position = find_key_word(original)\n",
        "  result = original[start_position+1 : end_position-1]\n",
        "  return result\n",
        "\n",
        "# using above functions to produce input\n",
        "def preprocessing(dataset):\n",
        "  dataset['replaced'] = dataset.apply(lambda x: extract_key_word(x['original']), axis = 1)\n",
        "  dataset['context']  = dataset.apply(lambda x : replace_key_word(x['original'],x['edit']), axis = 1)\n",
        "  dataset['add_humour'] = dataset.apply(lambda x:x['context'] + ' [SEP] '+ x['edit'] + ' was humorous',axis=1)\n",
        "\n",
        "# preprocess our training, validation and testing data\n",
        "\n",
        "preprocessing(train_df)\n",
        "preprocessing(val_df)\n",
        "preprocessing(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkJ4SQm19dpo"
      },
      "source": [
        "# We define our training loop\n",
        "\n",
        "def train(train_loader, model, epochs, val_dataloader, tokenizer, optimizer, scheduler):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "    print(\"Training model.\")\n",
        "    best_rmse = 1\n",
        "    # begin training\n",
        "    for epoch in range(1, epochs+1):\n",
        "\n",
        "        # store prediction and ture label\n",
        "        pre_array = np.array([])\n",
        "        true_array = np.array([])\n",
        "\n",
        "        # record the progress of our training\n",
        "        counter = 0\n",
        "        model.train()\n",
        "\n",
        "        # for each batch we do\n",
        "        for batch in train_loader:\n",
        "          input_ids, attention_masks = collate_fn(list(batch[0]), tokenizer)\n",
        "          input_ids = input_ids.to(device)\n",
        "          attention_masks = attention_masks.to(device)\n",
        "          labels = batch[1].to(device)\n",
        "          model.zero_grad()  \n",
        "\n",
        "          # obtain output      \n",
        "          outputs = model(input_ids, attention_mask = attention_masks, labels = labels)\n",
        "          \n",
        "          # extract loss and logits(predicitons) from outputs\n",
        "          loss = outputs.loss\n",
        "          logits = outputs.logits\n",
        "\n",
        "          # add predictions and labels to array\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          labels = labels.cpu().numpy()\n",
        "          pre_array = np.append(pre_array,logits)\n",
        "          true_array = np.append(true_array,labels)\n",
        "\n",
        "          # update model parameters and do optimisation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "          # calculate training rmse\n",
        "          _,mse,rmse = model_performance(logits,labels)\n",
        "          counter += 1\n",
        "          \n",
        "\n",
        "          # report the progress of the training and validation results\n",
        "          if counter % 20 == 0:\n",
        "              val_rmse, avg_val_loss = eval(val_loader, model, tokenizer)\n",
        "              print(f'|Epoch: {epoch} |Batch: {counter} | Total Batch: {len(train_loader)} | Train Loss: {loss:.4f} | Train RMSE: {rmse:.4f} | Val Loss: {avg_val_loss:.4f} | Val RMSE: {val_rmse:.4f} |')\n",
        "              if val_rmse < best_rmse:\n",
        "                best_rmse = val_rmse\n",
        "                torch.save(model.state_dict(), 'best_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aay8Omb4xbuW"
      },
      "source": [
        "# How we print the model performance\n",
        "\n",
        "def model_performance(output, target):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "    sq_error = (output - target)**2\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    return sse, mse, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWevPvP99dpo"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "\n",
        "def eval(val_loader, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    # enter evaluation model\n",
        "    model.eval()\n",
        "\n",
        "    # store model prediction and true label\n",
        "    sum_loss = 0\n",
        "    pre_array = np.array([])\n",
        "    true_array = np.array([])\n",
        "\n",
        "    for batch in val_loader:\n",
        "\n",
        "        # tokenize each batch\n",
        "        input_ids, attention_masks = collate_fn(list(batch[0]), tokenizer)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        # obtain output\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(input_ids, attention_mask=attention_masks, labels= labels)\n",
        "        \n",
        "        # extract loss from outputs\n",
        "        loss = outputs.loss\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        # extract logits(predictions) from outputs \n",
        "        logits = outputs.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        # add predictions and labels to array\n",
        "        pre_array = np.append(pre_array,logits)\n",
        "        true_array = np.append(true_array,labels)\n",
        "\n",
        "     # calculate validation rmse   \n",
        "    _,_,rmse = model_performance(pre_array,true_array)\n",
        "\n",
        "    avg_loss = sum_loss / len(val_loader)\n",
        "\n",
        "    return rmse, avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWzdLNRKQM_"
      },
      "source": [
        "# We evaluate performance on our test set\n",
        "\n",
        "def test(final_model, test_loader, tokenizer):\n",
        "  '''\n",
        "  Testing model performance on the dev set\n",
        "  '''\n",
        "  # evaluation model\n",
        "  final_model.eval()\n",
        "  pre_array = np.array([])\n",
        "  for batch in test_loader:\n",
        "    # obtain input_id and attention_mask\n",
        "    input_ids, attention_masks = collate_fn(list(batch), tokenizer)\n",
        "\n",
        "    # to device\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "\n",
        "    # obtain the output from final_model\n",
        "    with torch.no_grad():        \n",
        "        outputs = final_model(input_ids, attention_mask=attention_masks)\n",
        "    logits = outputs.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    # add prediction of each input\n",
        "    pre_array = np.append(pre_array,logits)\n",
        "\n",
        "  return pre_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5gTJmRq9dpp"
      },
      "source": [
        "## get input_id and attention_mask of each batch\n",
        "def collate_fn(batch, tokenizer):\n",
        "  '''\n",
        "  Tokenize our batch\n",
        "  '''\n",
        "  # list stored the input_id and attention_mask\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # implement tokenization for each row of a batch\n",
        "  for row in batch:\n",
        "    encodings = tokenizer(row, add_special_tokens = True, max_length = 32, return_tensors = 'pt', truncation=True, padding='max_length')\n",
        "    input_ids.append(encodings['input_ids'])\n",
        "    attention_masks.append(encodings['attention_mask'])\n",
        "\n",
        "  # convert list to tensor\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "\n",
        "\n",
        "## change dataframe to readable dataset\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, training = True):\n",
        "        # context = x\n",
        "        self.x_train = df['context']\n",
        "        # self.x_train = df['add_humour']\n",
        "        # if we are in the training phrase\n",
        "        self.training = training\n",
        "        if self.training:\n",
        "          # label = y\n",
        "          self.y_train = df['meanGrade']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if self.training:\n",
        "          return self.x_train[item], self.y_train[item]\n",
        "        else:\n",
        "          return self.x_train[item]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRXW0z89m6hj",
        "outputId": "d0adaab5-ed31-4353-d74f-025e3a0c203a"
      },
      "source": [
        "#### define our model\n",
        "\n",
        "## model can be Bert or Roberta\n",
        "\n",
        "## we are doing regression problem only having one label (num_labels = 1)\n",
        "\n",
        "# for bert\n",
        "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',  num_labels = 1)\n",
        "model_bert.cuda()\n",
        "model_bert = model_bert.double()\n",
        "\n",
        "# for roberta\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 1)\n",
        "model_roberta.cuda()\n",
        "model_roberta = model_roberta.double()\n",
        "\n",
        "\n",
        "optimizer_bert = AdamW(model_bert.parameters(),lr = learning_rate, eps = 1e-8)\n",
        "optimizer_roberta = AdamW(model_roberta.parameters(),lr = learning_rate, eps = 1e-8)\n",
        "\n",
        "# construct tokenizer for bert and roberta model\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "# construct dataset and dataloader\n",
        "train_dataset = Task1Dataset(train_df)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,  shuffle = True, batch_size = batch_size)\n",
        "val_dataset = Task1Dataset(val_df)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,  shuffle = False, batch_size = batch_size)\n",
        "test_dataset = Task1Dataset(test_df,False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle = False, batch_size = batch_size)\n",
        "\n",
        "# computer total steps\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler_bert = get_linear_schedule_with_warmup(optimizer_bert, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "scheduler_roberta = get_linear_schedule_with_warmup(optimizer_roberta, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WUxtiVI49dpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0b2662-b97e-43af-e6be-30914d4b44d4"
      },
      "source": [
        "##Â Approach 1 code, using functions defined above:\n",
        "# train for roberta\n",
        "train(train_loader, model_roberta, epochs, val_loader, tokenizer_roberta, optimizer_roberta, scheduler_roberta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "|Epoch: 1 |Batch: 20 | Total Batch: 302 | Train Loss: 0.5145 | Train RMSE: 0.7023 | Val Loss: 0.3438 | Val RMSE: 0.5862 |\n",
            "|Epoch: 1 |Batch: 40 | Total Batch: 302 | Train Loss: 0.3550 | Train RMSE: 0.6046 | Val Loss: 0.3237 | Val RMSE: 0.5689 |\n",
            "|Epoch: 1 |Batch: 60 | Total Batch: 302 | Train Loss: 0.4980 | Train RMSE: 0.7222 | Val Loss: 0.3426 | Val RMSE: 0.5848 |\n",
            "|Epoch: 1 |Batch: 80 | Total Batch: 302 | Train Loss: 0.3188 | Train RMSE: 0.5811 | Val Loss: 0.3299 | Val RMSE: 0.5738 |\n",
            "|Epoch: 1 |Batch: 100 | Total Batch: 302 | Train Loss: 0.4303 | Train RMSE: 0.6508 | Val Loss: 0.3054 | Val RMSE: 0.5524 |\n",
            "|Epoch: 1 |Batch: 120 | Total Batch: 302 | Train Loss: 0.2720 | Train RMSE: 0.5542 | Val Loss: 0.2911 | Val RMSE: 0.5392 |\n",
            "|Epoch: 1 |Batch: 140 | Total Batch: 302 | Train Loss: 0.3432 | Train RMSE: 0.6111 | Val Loss: 0.3168 | Val RMSE: 0.5621 |\n",
            "|Epoch: 1 |Batch: 160 | Total Batch: 302 | Train Loss: 0.2336 | Train RMSE: 0.5654 | Val Loss: 0.2864 | Val RMSE: 0.5343 |\n",
            "|Epoch: 1 |Batch: 180 | Total Batch: 302 | Train Loss: 0.2157 | Train RMSE: 0.5847 | Val Loss: 0.2782 | Val RMSE: 0.5266 |\n",
            "|Epoch: 1 |Batch: 200 | Total Batch: 302 | Train Loss: 0.2421 | Train RMSE: 0.5577 | Val Loss: 0.2817 | Val RMSE: 0.5302 |\n",
            "|Epoch: 1 |Batch: 220 | Total Batch: 302 | Train Loss: 0.2451 | Train RMSE: 0.5698 | Val Loss: 0.2881 | Val RMSE: 0.5361 |\n",
            "|Epoch: 1 |Batch: 240 | Total Batch: 302 | Train Loss: 0.3363 | Train RMSE: 0.6540 | Val Loss: 0.2953 | Val RMSE: 0.5432 |\n",
            "|Epoch: 1 |Batch: 260 | Total Batch: 302 | Train Loss: 0.2241 | Train RMSE: 0.6245 | Val Loss: 0.2816 | Val RMSE: 0.5301 |\n",
            "|Epoch: 1 |Batch: 280 | Total Batch: 302 | Train Loss: 0.2135 | Train RMSE: 0.6074 | Val Loss: 0.2855 | Val RMSE: 0.5334 |\n",
            "|Epoch: 1 |Batch: 300 | Total Batch: 302 | Train Loss: 0.2911 | Train RMSE: 0.5685 | Val Loss: 0.2919 | Val RMSE: 0.5392 |\n",
            "|Epoch: 2 |Batch: 20 | Total Batch: 302 | Train Loss: 0.2016 | Train RMSE: 0.6703 | Val Loss: 0.2809 | Val RMSE: 0.5292 |\n",
            "|Epoch: 2 |Batch: 40 | Total Batch: 302 | Train Loss: 0.1444 | Train RMSE: 0.7599 | Val Loss: 0.2915 | Val RMSE: 0.5396 |\n",
            "|Epoch: 2 |Batch: 60 | Total Batch: 302 | Train Loss: 0.2642 | Train RMSE: 0.7154 | Val Loss: 0.2769 | Val RMSE: 0.5255 |\n",
            "|Epoch: 2 |Batch: 80 | Total Batch: 302 | Train Loss: 0.1569 | Train RMSE: 0.6078 | Val Loss: 0.2731 | Val RMSE: 0.5217 |\n",
            "|Epoch: 2 |Batch: 100 | Total Batch: 302 | Train Loss: 0.2220 | Train RMSE: 0.8214 | Val Loss: 0.2747 | Val RMSE: 0.5234 |\n",
            "|Epoch: 2 |Batch: 120 | Total Batch: 302 | Train Loss: 0.1705 | Train RMSE: 0.6999 | Val Loss: 0.2865 | Val RMSE: 0.5348 |\n",
            "|Epoch: 2 |Batch: 140 | Total Batch: 302 | Train Loss: 0.2442 | Train RMSE: 0.8020 | Val Loss: 0.2816 | Val RMSE: 0.5301 |\n",
            "|Epoch: 2 |Batch: 160 | Total Batch: 302 | Train Loss: 0.2664 | Train RMSE: 0.6229 | Val Loss: 0.2813 | Val RMSE: 0.5298 |\n",
            "|Epoch: 2 |Batch: 180 | Total Batch: 302 | Train Loss: 0.1753 | Train RMSE: 0.6956 | Val Loss: 0.2806 | Val RMSE: 0.5291 |\n",
            "|Epoch: 2 |Batch: 200 | Total Batch: 302 | Train Loss: 0.2734 | Train RMSE: 0.6135 | Val Loss: 0.2728 | Val RMSE: 0.5217 |\n",
            "|Epoch: 2 |Batch: 220 | Total Batch: 302 | Train Loss: 0.1561 | Train RMSE: 0.6074 | Val Loss: 0.2708 | Val RMSE: 0.5196 |\n",
            "|Epoch: 2 |Batch: 240 | Total Batch: 302 | Train Loss: 0.1023 | Train RMSE: 0.6287 | Val Loss: 0.2718 | Val RMSE: 0.5206 |\n",
            "|Epoch: 2 |Batch: 260 | Total Batch: 302 | Train Loss: 0.1831 | Train RMSE: 0.7011 | Val Loss: 0.2727 | Val RMSE: 0.5214 |\n",
            "|Epoch: 2 |Batch: 280 | Total Batch: 302 | Train Loss: 0.1961 | Train RMSE: 0.6499 | Val Loss: 0.2721 | Val RMSE: 0.5209 |\n",
            "|Epoch: 2 |Batch: 300 | Total Batch: 302 | Train Loss: 0.1883 | Train RMSE: 0.6278 | Val Loss: 0.2729 | Val RMSE: 0.5217 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9MtHYL-Th2d"
      },
      "source": [
        "\n",
        "\n",
        "####################### implement model on test data (need test dataset with label)\n",
        "\n",
        "# true_df = pd.read_csv('test with label.csv') \n",
        "# model_roberta.load_state_dict(torch.load('best_model.pt'))\n",
        "# prediction_roberta = test(model_roberta, test_loader, tokenizer_roberta)\n",
        "# _, _, rmse_test_roberta = model_performance(prediction_roberta, np.array(true_df['meanGrade']))\n",
        "# print(rmse_test_roberta)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxFy7FEO6mfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa33448-e36d-4f8a-fef6-2fd3d562fb3e"
      },
      "source": [
        "# train for bert\n",
        "train(train_loader, model_bert, epochs, val_loader, tokenizer_bert, optimizer_bert, scheduler_bert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "|Epoch: 1 |Batch: 20 | Total Batch: 302 | Train Loss: 0.4002 | Train RMSE: 0.6331 | Val Loss: 0.3351 | Val RMSE: 0.5785 |\n",
            "|Epoch: 1 |Batch: 40 | Total Batch: 302 | Train Loss: 0.5267 | Train RMSE: 0.7263 | Val Loss: 0.3286 | Val RMSE: 0.5728 |\n",
            "|Epoch: 1 |Batch: 60 | Total Batch: 302 | Train Loss: 0.2042 | Train RMSE: 0.4775 | Val Loss: 0.3257 | Val RMSE: 0.5703 |\n",
            "|Epoch: 1 |Batch: 80 | Total Batch: 302 | Train Loss: 0.3630 | Train RMSE: 0.5864 | Val Loss: 0.3190 | Val RMSE: 0.5644 |\n",
            "|Epoch: 1 |Batch: 100 | Total Batch: 302 | Train Loss: 0.2311 | Train RMSE: 0.5226 | Val Loss: 0.3093 | Val RMSE: 0.5557 |\n",
            "|Epoch: 1 |Batch: 120 | Total Batch: 302 | Train Loss: 0.1876 | Train RMSE: 0.4921 | Val Loss: 0.2965 | Val RMSE: 0.5441 |\n",
            "|Epoch: 1 |Batch: 140 | Total Batch: 302 | Train Loss: 0.1956 | Train RMSE: 0.5010 | Val Loss: 0.2928 | Val RMSE: 0.5407 |\n",
            "|Epoch: 1 |Batch: 160 | Total Batch: 302 | Train Loss: 0.3536 | Train RMSE: 0.6163 | Val Loss: 0.2915 | Val RMSE: 0.5392 |\n",
            "|Epoch: 1 |Batch: 180 | Total Batch: 302 | Train Loss: 0.3308 | Train RMSE: 0.6347 | Val Loss: 0.2943 | Val RMSE: 0.5420 |\n",
            "|Epoch: 1 |Batch: 200 | Total Batch: 302 | Train Loss: 0.2642 | Train RMSE: 0.6461 | Val Loss: 0.2867 | Val RMSE: 0.5349 |\n",
            "|Epoch: 1 |Batch: 220 | Total Batch: 302 | Train Loss: 0.2956 | Train RMSE: 0.6185 | Val Loss: 0.2856 | Val RMSE: 0.5340 |\n",
            "|Epoch: 1 |Batch: 240 | Total Batch: 302 | Train Loss: 0.2724 | Train RMSE: 0.5973 | Val Loss: 0.2861 | Val RMSE: 0.5345 |\n",
            "|Epoch: 1 |Batch: 260 | Total Batch: 302 | Train Loss: 0.3582 | Train RMSE: 0.6949 | Val Loss: 0.2830 | Val RMSE: 0.5316 |\n",
            "|Epoch: 1 |Batch: 280 | Total Batch: 302 | Train Loss: 0.1495 | Train RMSE: 0.4871 | Val Loss: 0.2784 | Val RMSE: 0.5272 |\n",
            "|Epoch: 1 |Batch: 300 | Total Batch: 302 | Train Loss: 0.2757 | Train RMSE: 0.5891 | Val Loss: 0.2787 | Val RMSE: 0.5274 |\n",
            "|Epoch: 2 |Batch: 20 | Total Batch: 302 | Train Loss: 0.3819 | Train RMSE: 0.7372 | Val Loss: 0.3309 | Val RMSE: 0.5752 |\n",
            "|Epoch: 2 |Batch: 40 | Total Batch: 302 | Train Loss: 0.1616 | Train RMSE: 0.6143 | Val Loss: 0.2882 | Val RMSE: 0.5363 |\n",
            "|Epoch: 2 |Batch: 60 | Total Batch: 302 | Train Loss: 0.2826 | Train RMSE: 0.7061 | Val Loss: 0.2797 | Val RMSE: 0.5284 |\n",
            "|Epoch: 2 |Batch: 80 | Total Batch: 302 | Train Loss: 0.2373 | Train RMSE: 0.7644 | Val Loss: 0.2874 | Val RMSE: 0.5359 |\n",
            "|Epoch: 2 |Batch: 100 | Total Batch: 302 | Train Loss: 0.1689 | Train RMSE: 0.7073 | Val Loss: 0.2910 | Val RMSE: 0.5392 |\n",
            "|Epoch: 2 |Batch: 120 | Total Batch: 302 | Train Loss: 0.2153 | Train RMSE: 0.5865 | Val Loss: 0.2960 | Val RMSE: 0.5439 |\n",
            "|Epoch: 2 |Batch: 140 | Total Batch: 302 | Train Loss: 0.2070 | Train RMSE: 0.5831 | Val Loss: 0.2815 | Val RMSE: 0.5302 |\n",
            "|Epoch: 2 |Batch: 160 | Total Batch: 302 | Train Loss: 0.1666 | Train RMSE: 0.6710 | Val Loss: 0.2804 | Val RMSE: 0.5292 |\n",
            "|Epoch: 2 |Batch: 180 | Total Batch: 302 | Train Loss: 0.2324 | Train RMSE: 0.7578 | Val Loss: 0.2815 | Val RMSE: 0.5302 |\n",
            "|Epoch: 2 |Batch: 200 | Total Batch: 302 | Train Loss: 0.3580 | Train RMSE: 0.8120 | Val Loss: 0.2882 | Val RMSE: 0.5366 |\n",
            "|Epoch: 2 |Batch: 220 | Total Batch: 302 | Train Loss: 0.1369 | Train RMSE: 0.7010 | Val Loss: 0.2961 | Val RMSE: 0.5438 |\n",
            "|Epoch: 2 |Batch: 240 | Total Batch: 302 | Train Loss: 0.2309 | Train RMSE: 0.6903 | Val Loss: 0.2864 | Val RMSE: 0.5349 |\n",
            "|Epoch: 2 |Batch: 260 | Total Batch: 302 | Train Loss: 0.2214 | Train RMSE: 0.7563 | Val Loss: 0.2872 | Val RMSE: 0.5356 |\n",
            "|Epoch: 2 |Batch: 280 | Total Batch: 302 | Train Loss: 0.1745 | Train RMSE: 0.5943 | Val Loss: 0.2868 | Val RMSE: 0.5352 |\n",
            "|Epoch: 2 |Batch: 300 | Total Batch: 302 | Train Loss: 0.2385 | Train RMSE: 0.6971 | Val Loss: 0.2849 | Val RMSE: 0.5334 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W573ItJ-UJAI"
      },
      "source": [
        "####################### implement model on test data (need test dataset with label)\n",
        "\n",
        "# true_df = pd.read_csv('test with label.csv') \n",
        "# prediction_bert = test(model_bert, test_loader, tokenizer_bert)\n",
        "# _, _, rmse_test_bert = model_performance(prediction_bert, np.array(true_df['meanGrade']))\n",
        "# print(rmse_test_bert)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}